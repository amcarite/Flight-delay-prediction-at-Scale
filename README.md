# Flight-delay-prediction-at-Scale Report

This report discuss the problem being addressed, results, and conclusion. Two notebooks are also provided in this repo that displays the code used to determine the results. 

### Abstract

Mitigating the logistical and financial impact of airport delays has never been more important for aviation logistics companies to compete and succeed, and the new forecasting tool from the team can give these companies 2 hours advance notice if a flight will be delayed by at least 15 minutes, and do so with AUC-ROC performance of over 80.7% (on balanced delay/no-delay samples).  The team accomplished this feat by creating a data pipeline of flight, weather, and airport data, which when processed, feeds a logistic regression model with a log-loss cost function.  Blocked time-series cross validated data, rebalanced by undersampling non-delayed flights from 2015-2020, is used to train the model, and 2021 data is used for testing.  While features used for the model were selected from the original data pipeline via linear regression using L1 regularization, additional features were engineered, such as the frequency of delays from the same airport in earlier time-windows, the pagerank of the airport graph and an airline ranking.  In the final logistic regression model, grid search was used to tune the regularization parameter.  Finally, the results were compared to an XGBoost ensemble model.



|     |Baseline|Logistic|XGBoost|
|-----|--------|--------|-------|
|Validation Data AUC|    0.5    |    0.81    |    0.761   |
|Testing Data AUC |    0.5    |    0.8065    |    0.7597   |

### Primary Datasets

Data for the model was drawn primarily from 3 datasets: an airline performance dataset and airport dataset from the U.S. Department of Transportation, and a weather dataset from NOAA.  These were supplemented by a fourth dataset from the International Air Transport Association (IATA) which published a list of corresponding 3-character airport codes and 4-character airport codes.

More specifically, the Airline On-Time Performance Data comes from the TranStats data collection from the U.S. Department of Transportation, and covers “US certified air carriers that account for at least 1% of domestic scheduled passenger revenues” from 2015 through 2021, comprising 74,177,433 rows x 109 columns in dimension.  This data covers several categories of information: datetime information for scheduled and actual departure, arrival, and other activities such as wheels on and wheels off the ground; delays and type of delays; carrier and plane information; and origin and destination information.  The import of the data can be seen in Step 1 below.  [Preliminary EDA](https://github.com/amcarite/Flight-delay-prediction-at-Scale/blob/main/flight_delays_eda.ipynb) as well as Step 2 below shows that there are a mix of numeric and categorical features.  As a whole, the dataset is fairly complete, although many data fields are populated by NaN because they track relatively rare events, like the delay caused by a mechanical error, or the time spent on the ground at a diverted airport.  However, this appears to be by design and not by incomplete or missing information.

As a whole, there are some noteworthy characteristics of the dataset.  First, there are a large number of fields for describing the performance of flights that are diverted in sequence to a series of other airports, but these are rare occurrences, and were ultimately dropped from our final model.  Although many features in the flights dataset measure aspects of time, most numeric time-related delays are not normal—many appear to be Poisson distributions.  It is also important to note that time is recorded in local time, which means that it will need to be converted before joining it with data that is recorded in UTC standard times.

Weather Data comes from the National Oceanic and Atmospheric Administration repository’s Integrated Surface Data (ISD) from 2015 through 2021, and has 898,983,399 rows x 124 columns.  The NOAA dataset is very complex, has many missing or incomplete fields.  The dataset contains several categories of information: date-time information; weather station location information; the source of the information (often METAR or another source that automatically generates observations at a fixed interval); scientific weather measurements and weather descriptions (wind, pressure, temperature); sky and visibility descriptions; precipitation measurements and descriptions; and other status fields for the sensory equipment.

Although the initial EDA of the weather data revealed that most weather-related fields are actually NaN, the REM (the “remarks”) field was consistently populated with METAR data, a coded weather report from the observation station.  Parsing this field enables a more complete characterization of the weather information, even if NaNs are common in other fields of the weather dataset.  One notable complication to the weather dataset is that while the data is collected at regular intervals, the timestamps do not match up exactly with the flight departure times. To account for this, first the UTC timestamps were stripped from the weather data, and adjusted to local time using the supplementary IATA dataset. Then, a two-hour prior-to-departure time was added to the airlines dataset, and weather data was joined such that the row with the four-character airport code that was closest to the two-hour-prior-to-scheduled-departure was matched to the airline dataset.  This last join took our Spark cluster 22 minutes to complete.

#### Data Pre-Processing
To prepare the data for our model, a number of pre-processing steps were required.  First, several engineered variables were added with the intent of improving our performance metrics while at the same time reducing the number of redundant features.  (See Step 4 below).  One category of variables was time-windowing, essentially looking to see what percentage of flights leaving the same airport (and separately, going to the same destination) in the previous 4 hours that were delayed.  Initially, we experimented with 12-hour look back windows, but found that 4-hour windows improved AUC scores by approximately 5% at that particular time of model analysis.

In addition, we created a PageRank feature to determine which airports are the most associated with delays. Initially, we had created the PageRank score that represented all years of the data. However, we realized that this would lead to data leakage, and avoiding it was a complex endeavor.  As a solution, we divided our dataset for a blocked time-series analysis.  Of the training set of 2015-2020, we created 6 folds.  In each fold, only the delayed January through November flights in each year were counted, and these were used as initial weights for the airport nodes in our graph. We revised our approach to run PageRank for each year between 2015-2020 (excluding December data for each respective year). Subsequently, we took these computed PageRank scores and averaged them to create the PageRank score for 2021. Lastly, we joined the PageRank score back to the main table to have a score for each year and airport.

We also created a feature that ranks airlines by delays.  We took the original flights data (not the post-joined data) to ensure that we have all of the flights data and nothing is dropped.  Then we filtered to identify flights that were delayed but the delay was partly attributable to the carrier, e.g. CARRIER_DELAY is greater than 0.  Rather that taking the number of delays by flights, which gives a skewed view, we calculated for each flight the percentage of delays (flights delayed divided by total flights.).  We then ranked the percentage of delays to create our ranking feature.  We used a similar treatment for data leakage as PageRank.

At this point, we constructed a model pipeline in Step 5 that would be positioned handle any inputted data.  Our model absorbs the raw features provided from Steps 1-4, and then scales the data with a Standard Scaler to ensure that all features are between the ranges of zero and one, and that our categorical variables are one-hot encoded.  At this stage, our model had just over 869 features.  For each row (data point), Spark then created an 869-point vector, and each of its values were between 0 and 1. (See Step 6.)

### In-Pipeline Data Processing

Because fitting a model on 869 features would be very computationally intense, we elected to reduce our variables via linear regression with L1 regularization (see Step 7).  This process was more complex than we anticipated because Spark's preference for dealing with vectors rather than dataframes meant that we had to carefully feed the newly created feature vector into the model and then carefully only slice out the indices that we were to keep.  Experimentation revealed that an L1 regularization parameter of 0.01 yielded 3 important features. When we first ran L1 regularization, we used a regularization parameter of 0.1 which resulted in no features. Although 3 features may seem small, the three features that are kept are our engineered features 'frac_departures_to_destination_delayed_prev_4_hours', 'frac_origin_departures_delayed_prev_4_hours', 'Airline_Delay_Ranking', and we decided this number of features would be sufficient because they combine aspects of other features.

Our initial EDA indicated that our data was unbalanced--over 80% of the flights had no delays, and less than 20% of the model had delays.  This imbalance hinders our ability to train our model for the delay scenario, and consequently we attempted to rebalance our training data before we trained our model.  There are two general approaches to solve this problem--oversample the delay examples (SMOTE), or undersample the no-delay examples. Given that fitting our model was already computationally challenging for our cluster resources, and the additional challenges created by implementing SMOTE, we opted for the undersampling approach.

Splitting the time-series data in order to fit our logistic regression model required great care to avoid data leakage.  Although 2021 data continued to be held out for final testing, the training data in 2015 through 2020 was split with a blocked time-series cross-validation approach.  Each year was applied to a separate fold (6 folds in total), and January through November data was treated as the training data for the fold, and was tested against the December data for that year.

### Data Leakage Discussion

The definition of data leakage is when information from outside the training dataset is used to create the model, and this problem requires careful scrutiny when working with time-series data. In time series analysis, this can mean making a prediction based on data that is from the future at the time of prediction.

The first step we took in ensuring our pipeline has no data leakage was creating a time-series based cross validation strategy. For non-time-series, it is customary to randomly assign data to either the training or validation sets.  However, this process would not work for time-series data because it would allow data for training to be used for prediction of the validataion set even though it occurs after the validation data. To solve this problem, we decided to implement a blocked cross-validation method.

Time-series analysis was a new activity for the members of our group and initially we made some mistakes with our engineered features. The first mistake we made was calculating PageRank for the full 6-year training period. To fix this, we calculated PageRank for each fold in our cross validation split to ensure no leakage. Ensuring that there was no data leakage was difficult, and we realized that even basic statistics about our dataset, such as the delay/no-delay ratio that we used for rebalancing, actually required calculation in each fold of our blocked time-series cross validation to ensure that the 2020 delay/no-delay sampling ratio did not improperly contaminate the 2015 delay/no-delay sampling ratio.


### Baseline Model, Logistic Regression Model, and XGBoost Model
Because the ratio of no-delay to delay across the entire dataset was approximately 4:1, by simply predicting the that all flights would be in the no-delay category, we would correctly classify flights the majority of the time.  This "baseline" model, while not sophisticated, yields an AUC of approximately 0.50 and serves as a baseline against which we can measure improvements with better models. 

We ran two different types of more advanced models in order to predict flight delays.  The first was a logistic regression model, which is a relatively simple model that is very straightforward to train and handles binary classification challenges relatively well.  However, our primary hyperparameter tuning option was through the regularization parameter, which limited the performance we could produce.  We also experimented with tuning the ElasticNet parameter of the regularization, which uses a hybrid of Lasso and Ridge regression, in order optimize results. However, this experimentation did not lead to any significant changes to the results.

Additionally, we also ran a more sophisticated XGBoost model, which has shown very good results in Kaggle competitions which require binary classification. The XGBoost algorithm improves upon the already powerful tree ensemble by incorporating gradient descent architecture to boost weak learners. XGBoost works great with spark as it can take advantage of distributed computing. Based on several experiments, although the AUC with the more sophisticated XGBoost model did not increase beyond the 0.75-0.76 range, running it on a much larger feature provided very similar results, suggesting that our L1 regression was successful at correctly selecting features while making our algorithms run much faster by reducing unneeded features.

Our trivial baseline model will have no loss function or regularization as it is simply always predicting “no delay”. However, we will use a loss function for the two other models that we expect to build. For logistic regression, the team plans to use Log Loss as this is the standard practice when running logistic classification problems:

$$
Log Loss = \frac{-1}{N}\sum_{i=1}^{N}\sum_{j=1}^{M}x_{ij}*log(p_{ij})
$$

where N is the number of samples, M is the number of features, indicates whether ith sample belongs to jth class or not and p indicates the probability of the ith sample belonging to the jth class. We also used the elastic net regularization, a hybrid method which aims to not only minimize the amount of strongly correlated variables but also to make the loss function strongly convex:

$$
ElasticNetReg = (1-\alpha)|\omega_{1}| + \alpha|\omega_{2}^{2}|
$$

where  is the regularization term (hyper parameter) and w is the sum of the absolute value of the magnitude of the coefficients.
 
For our tree ensemble model, we had a bit more flexibility in choosing our loss function, and we used the logistic regression loss function and the traditional regularization term used in most XGBoost models:

$$
XGBoostRegularizationTerm = \gamma T+\frac{1}{2}\sum_{j=1}^{T}\omega_{j}^{2}
$$

where w is the vector of leaf scores, T is the number of leaves, and second term is the regularization term.

### Results and Gap Analysis

Our models were evaluated with the Area Under the Curve - Receiver Operating Characteristics (AUC-ROC) metric because it works well with classification algorithms, especially with unbalanced outcome variables such as ours. On the 6-fold cross validation, the AUC of the baseline model was 0.50, the AUC of the tuned logistic regression model as 0.82, and the AUC of the tuned XGBoost model was 0.761 for our best run.  On the final test set, the AUC of the baseline model was 0.50, the AUC of the tuned logistic regression model as 0.81, and the AUC of the tuned XGBoost model was 0.76.  This demonstrates that our models have solid predictive power even though our model outputs seemed relatively insensitive to hyperparameter tuning.  Additionally, our L1 regularization only kept 3 features which means this may have been excessively aggressive.  We would pursue additional exploration of this hyperparameter with additional time resources at our disposal.

It is important to note that in our cross-validation studies, the AUC was the average composite of all 6 folds.  However, we noticed that the last year of the study 2020, scored consistently higher than the other folds, no matter which model we used.  Our intuition is that the 2020 was a year in which travel was greatly impacted by the pandemic, and with fewer travelers, airlines had extra capacity and thus fewer cascading delays.  Because cascading delays involve a chain of events and are harder to predict, our models may have performed better when there were fewer of them and had more direct, generic causes.

For our gap analysis, we compared our AUC metric to all other groups that reported this metric at the time of writing this analysis (11:16pm Eastern Time 8/4). Groups reported AUC scores of 0.61, 0.68, 0.65, and 0.60. This means that our best score of 0.81 puts us at the top of all groups that decided to use AUC as their metric. The next closest is 0.13 AUC score away. This validates that our developed featuers were highly predictive. 

### Learnings and Discussion

With the benefit of hindsight, if we had to do the project again, there are many areas where we would take a different approach.  Although avoiding the technical mistakes that created difficulties early on would certainly have been helpful, improvements to our model-building process likely would have resulted in even better outcomes.

First, although we performed what we thought was a thorough EDA in the early phases of our project, only in later phases did we fully appreciate the importance of doing EDA at every step.  For instance, in addition to the data we received from databases, we should have run an EDA on each new feature that we engineered.  By stress-testing each of our engineered features as thoroughly as we vetted the input features, we would have had better data with which to compare different versions of each engineered feature.  Also, we after Phase 1 and 2, which required substantial EDA, we elected to use the class-provided joined dataset instead of our own, because we thought that it would save us time because the METAR weather variables were already parsed.  However, many of the features in the provided joined dataframe were different than the ones we created in Phases 1 and 2, and we should have rerun an EDA on the new joined dataframe.  Several pre-processing functions that we created to work on our original version of the dataframe did not work as planned on the provided one, and that took us a lot more time than we expected to debug.

After Phases 1 and 2, we were very excited to have a complete, joined dataset that we saved to the blob, but we were mistaken in thinking that writing something to the blob signifies that we had completed something.  Rather than using this as a starting point or floor for our subsequent work, we would have been much better off if we had gone back to the smaller, 3-month dataset that we originally used, and perfected building our data processing pipeline and model on this smaller dataset.  Had we done that, we could have iterated much faster and performed more experiments, rather than waiting for our cluster to churn through large datasets.

Part of the challenge was that we tried to include as many features as possible in the pipeline in order to be thorough, with the expectation that a linear regression with L1 regularization would help us winnow down our features to just the key features.  There were several problems with this let-the-model-figure-it-out approach.  First, there were so many features that we didn't scrutinize each one carefully enough--our pipeline mistakenly classified many binary encoded features as categorical, and therefore exploded them further into unnecessary separate features. Second, just because would could create one-hot-encoded variables from categorical features like airport_codes and tail_numbers does not mean that we should.  We could have tested to see if a subset of these features was useful before devoting considerable computational resources to find needles in haystacks.  Had we thought more clearly, rather than trying to one-hot encode tail numbers, we could have been more creative about how we incorporated the tail number information into our model, like how perhaps may have been better utilized as part of an engineered graph-like feature rather than a one-hot encoded one.

Because we had a lot of training in L1 regularization and felt comfortable with using it, we did not spend enough time exploring if there were better methods for systematic feature selection in a dataset this large.  Consequently, we did not spend enough time looking at many other techniques which may have been more efficient.  Since feature selection was a significant part of our computational expenditure, we would have been well served by finding a more efficient process.  Many features that we anticipated would be significant, like pagerank, did not survive the L1 regularization, and we would have liked to have explored why if we had additional time and resources.  Furthermore, we would have liked to experiment further with many more folds that our current 6-fold process. 

For the members of our group, this was the largest data science project that we had tackled as a team.  If we had more time, we definitely would have invested more tools to measure our performance and collaborate more effectively.  With each team member working in his own version of the notebook, on a different parts of the pipeline, on different subsets of features, it was difficult to run experiments that generated apples-to-apples results.  In retrospect, it may been much more effective to get an end-to-end pipeline running first on a small dataset with a small number of features, and then backfill pipeline activites like feature selection on larger datasets.

While this was not something that we could control in this particular project, having full control of our cluster could have made many of our tasks easier.  For instance, if we had greater ability to adjust the number of workers in our cluster, and tune other settings, we think we may have been able to get some of our tasks to run more effectively. 

If we had more time, there are many improvements that we would make to our analysis.  One of the most important would be a deeper dive into feature analysis and the relative importance of each of the features that we kept.  L1 Regularization selected features that aligned with many of our expectations for what might be important features, but we did not discover a method of ranking the feature importance in our current implementation.  Having that data would be useful not only to our audience, but it would provide clues on where to focus future efforts.  In a similar vein, we wished that we had done more correlation studies between features.  If our goal is to provide means for clients to mitigate the effects of delays, they would need to know how much impact an action on an individual variable would have.

### Conclusion

Predicting airport delays is crucial for aviation logistics companies to be competitive. The team has set out to develop a tool that can give these companies 2 hours notice if a flight will be delayed by 15 or more minutes. We hypothesized that a machine learning pipeline with unique features can accurately predict flight delays and we have proven this with our pipeline showcased above. Some of the interesting aspects about our pipeline include the engineering of novel features like airport pagerank and time window analysis, feature selection using L1 regularization, and blocked time-series cross validation. This resulted in an AUC of 81%. For our modeling, we compared a logistic regression model, and an XGBoost model to our baseline and we improved our baseline by 21% AUC. This put us as the top AUC score at the time of checking.
